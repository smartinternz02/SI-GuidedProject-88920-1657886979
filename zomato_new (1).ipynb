{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "787ec031",
   "metadata": {
    "id": "787ec031"
   },
   "outputs": [],
   "source": [
    "#import Libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5fe5654",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, types\n",
    "import pandas as pd\n",
    "from botocore.client import Config\n",
    "import ibm_boto3\n",
    "\n",
    "def __iter__(self): return 0\n",
    "\n",
    "# @hidden_cell\n",
    "# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n",
    "# You might want to remove those credentials before you share the notebook.\n",
    "client_a51976db99ed4e449404373b785ef6f1 = ibm_boto3.client(service_name='s3',\n",
    "    ibm_api_key_id='t4qXhrfHX9hiJ-fjcnENjDxnRCm3C5dnsD5HbaiVj_nN',\n",
    "    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/oidc/token\",\n",
    "    config=Config(signature_version='oauth'),\n",
    "    endpoint_url='https://s3.private.us.cloud-object-storage.appdomain.cloud')\n",
    "\n",
    "streaming_body_1 = client_a51976db99ed4e449404373b785ef6f1.get_object(Bucket='sentimentanalysis-donotdelete-pr-znigxradjugep9', Key='zomato (2).zip')['Body']\n",
    "\n",
    "# Your data file was loaded into a botocore.response.StreamingBody object.\n",
    "# Please read the documentation of ibm_boto3 and pandas to learn more about the possibilities to load the data.\n",
    "# ibm_boto3 documentation: https://ibm.github.io/ibm-cos-sdk-python/\n",
    "# pandas documentation: http://pandas.pydata.org/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1df42727",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "import zipfile \n",
    "unzip=zipfile.ZipFile(BytesIO(streaming_body_1.read()),'r')\n",
    "file_paths= unzip.namelist()\n",
    "for path in file_paths:\n",
    "    unzip.extract(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "746dfd73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/wsuser/work'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f24ebfaf",
   "metadata": {
    "id": "f24ebfaf",
    "outputId": "23997a6b-ff2e-4837-e97c-1e425f9affff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Rated 4.0', 'RATED\\n  A beautiful place to dine in.The interiors take you back to the Mughal era. The lightings are just perfect.We went there on the occasion of Christmas and so they had only limited items available. But the taste and service was not compromised at all.The only complaint is that the breads could have been better.Would surely like to come here again.'), ('Rated 4.0', 'RATED\\n  I was here for dinner with my family on a weekday. The restaurant was completely empty. Ambience is good with some good old hindi music. Seating arrangement are good too. We ordered masala papad, panner and baby corn starters, lemon and corrionder soup, butter roti, olive and chilli paratha. Food was fresh and good, service is good too. Good for family hangout.\\nCheers'), ('Rated 2.0', 'RATED\\n  Its a restaurant near to Banashankari BDA. Me along with few of my office friends visited to have buffet but unfortunately they only provide veg buffet. On inquiring they said this place is mostly visited by vegetarians. Anyways we ordered ala carte items which took ages to come. Food was ok ok. Definitely not visiting anymore.'), ('Rated 4.0', 'RATED\\n  We went here on a weekend and one of us had the buffet while two of us took Ala Carte. Firstly the ambience and service of this place is great! The buffet had a lot of items and the good was good. We had a Pumpkin Halwa intm the dessert which was amazing. Must try! The kulchas are great here. Cheers!'), ('Rated 5.0', 'RATED\\n  The best thing about the place is itÃ\\x83Ã\\x83Ã\\x82Ã\\x82Ã\\x83Ã\\x82Ã\\x82Ã\\x92s ambiance. Second best thing was yummy ? food. We try buffet and buffet food was not disappointed us.\\nTest ?. ?? ?? ?? ?? ??\\nQuality ?. ??????????.\\nService: Staff was very professional and friendly.\\n\\nOverall experience was excellent.\\n\\nsubirmajumder85.wixsite.com'), ('Rated 5.0', 'RATED\\n  Great food and pleasant ambience. Expensive but Coll place to chill and relax......\\n\\nService is really very very good and friendly staff...\\n\\nFood : 5/5\\nService : 5/5\\nAmbience :5/5\\nOverall :5/5'), ('Rated 4.0', 'RATED\\n  Good ambience with tasty food.\\nCheese chilli paratha with Bhutta palak methi curry is a good combo.\\nLemon Chicken in the starters is a must try item.\\nEgg fried rice was also quite tasty.\\nIn the mocktails, recommend \"Alice in Junoon\". Do not miss it.'), ('Rated 4.0', 'RATED\\n  You canÃ\\x83Ã\\x83Ã\\x82Ã\\x82Ã\\x83Ã\\x82Ã\\x82Ã\\x92t go wrong with Jalsa. Never been a fan of their buffet and thus always order alacarteÃ\\x83Ã\\x83Ã\\x82Ã\\x82Ã\\x83Ã\\x82Ã\\x82Ã\\x92. Service at times can be on the slower side but food is worth the wait.'), ('Rated 5.0', 'RATED\\n  Overdelighted by the service and food provided at this place. A royal and ethnic atmosphere builds a strong essence of being in India and also the quality and taste of food is truly authentic. I would totally recommend to visit this place once.'), ('Rated 4.0', 'RATED\\n  The place is nice and comfortable. Food wise all jalea outlets maintain a good standard. The soya chaap was a standout dish. Clearly one of trademark dish as per me and a must try.\\n\\nThe only concern is the parking. It very congested and limited to just 5cars. The basement parking is very steep and makes it cumbersome'), ('Rated 4.0', 'RATED\\n  The place is nice and comfortable. Food wise all jalea outlets maintain a good standard. The soya chaap was a standout dish. Clearly one of trademark dish as per me and a must try.\\n\\nThe only concern is the parking. It very congested and limited to just 5cars. The basement parking is very steep and makes it cumbersome'), ('Rated 4.0', 'RATED\\n  The place is nice and comfortable. Food wise all jalea outlets maintain a good standard. The soya chaap was a standout dish. Clearly one of trademark dish as per me and a must try.\\n\\nThe only concern is the parking. It very congested and limited to just 5cars. The basement parking is very steep and makes it cumbersome')]\n"
     ]
    }
   ],
   "source": [
    "#First import the dataset in the dataset variable\n",
    "#data_review variable contains the reveiws and ratings \n",
    "dataset = pd.read_csv(\"zomato.csv\")\n",
    "data_review = dataset['reviews_list']\n",
    "\n",
    "print(dataset['reviews_list'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b660c854",
   "metadata": {
    "id": "b660c854"
   },
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c689df98",
   "metadata": {
    "id": "c689df98"
   },
   "outputs": [],
   "source": [
    "# here we tokenize the rating string and the review string\n",
    "#loop over all the rows\n",
    "for row_num in range(0,51717):\n",
    "    # split the revie text at '()\n",
    "    lst = data_review[row_num].split(\"('\")\n",
    "    for i in lst:\n",
    "        if len(i) > 5:\n",
    "            if i.find(\"',\") != -1:\n",
    "                single_rev = i.split(\"',\")\n",
    "                if len(single_rev[0]) > 2:\n",
    "                    x.append(single_rev[0])\n",
    "                if len(single_rev[1]) > 2:    \n",
    "                    y.append(single_rev[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95545bfe",
   "metadata": {
    "id": "95545bfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 19.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: click in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from nltk) (0.17.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from nltk) (2021.11.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from nltk) (4.62.3)\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.7\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8223c1c",
   "metadata": {
    "id": "b8223c1c",
    "outputId": "4329987e-0714-4e11-8229-ea99780dc126"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/wsuser/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "#Import the Librariesimport re \n",
    "import nltk\n",
    "nltk.download(\"stopwords\") \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cdfd667",
   "metadata": {
    "id": "0cdfd667"
   },
   "outputs": [],
   "source": [
    "# to store the final rating\n",
    "rating_final = []\n",
    "# to store cleaned revies\n",
    "review_final = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b639a0d",
   "metadata": {
    "id": "3b639a0d"
   },
   "outputs": [],
   "source": [
    "# the rating string contains words and numbers \n",
    "# so we tokenize the numbers only from it and change into float\n",
    "# for rartings below 2.5 we store the rating as poor\n",
    "# for ratings between 2.5 and 3.5 the rating as average\n",
    "# for ratings more than 3.5 tha rating stored as good\n",
    "\n",
    "for loop in range(0,40000):\n",
    "    data_x = x[loop]\n",
    "    data_x = re.sub('[a-zA-Z]', \" \", data_x)\n",
    "    data_x = data_x.split()\n",
    "    data_x = ''.join(data_x)\n",
    "    data_x = float(data_x)\n",
    "    if data_x < 2.5:\n",
    "        rating_final.append(\"poor\") #poor\n",
    "    elif data_x >= 2.5 and data_x <= 3.5 :    \n",
    "        rating_final.append(\"average\") # average\n",
    "    elif data_x > 3.5:\n",
    "        rating_final.append(\"good\") #good\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa8815ea",
   "metadata": {
    "id": "fa8815ea",
    "outputId": "1e69d796-e51e-4281-d65b-328a794c6526"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from sklearn) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.20.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from scikit-learn->sklearn) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from scikit-learn->sklearn) (0.17.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.7.3)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1309 sha256=ee006b349f9a9b25e62c55f78795c7ee2e1fdefc0cc315ce1aeda2b4a70682e0\n",
      "  Stored in directory: /tmp/wsuser/.cache/pip/wheels/e4/7b/98/b6466d71b8d738a0c547008b9eb39bf8676d1ff6ca4b22af1c\n",
      "Successfully built sklearn\n",
      "Installing collected packages: sklearn\n",
      "Successfully installed sklearn-0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc115ca5",
   "metadata": {
    "id": "bc115ca5"
   },
   "outputs": [],
   "source": [
    "# label encode the Ratings and OneHotEncode for the classification        \n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "rating_final = le.fit_transform(rating_final)\n",
    "\n",
    "rating_final = np.array(rating_final)\n",
    "rating_final = np.expand_dims(rating_final, axis=1)\n",
    "        \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "one = OneHotEncoder()\n",
    "rates = one.fit_transform(rating_final).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bdc69493",
   "metadata": {
    "id": "bdc69493"
   },
   "outputs": [],
   "source": [
    "# Here the unnecessary stop words from the reviews lists\n",
    "# Stemming operations are also done here \n",
    "\n",
    "for loop in range(0,40000) : \n",
    "    data_y = y[loop]\n",
    "    data_y = re.sub('[^a-zA-Z]', \" \", data_y)\n",
    "    data_y = data_y.lower()\n",
    "    data_y = data_y.split()\n",
    "    data_y = [ps.stem(word) for word in data_y if not word in set(stopwords.words('english'))]\n",
    "    data_y = ' '.join(data_y)\n",
    "    review_final.append(data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a2262f5",
   "metadata": {
    "id": "0a2262f5"
   },
   "outputs": [],
   "source": [
    "# count vectorize the reviews according to the unique words    \n",
    "                    \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features = 20000)\n",
    "x_final = cv.fit_transform(review_final).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5e74e06",
   "metadata": {
    "id": "a5e74e06"
   },
   "outputs": [],
   "source": [
    "# saving the vectorizer which would be used as dictionary.\n",
    "import pickle\n",
    "pickle.dump(cv, open('cv.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5fb4497c",
   "metadata": {
    "id": "5fb4497c"
   },
   "outputs": [],
   "source": [
    "# Split the data into test and train sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_final,rates, test_size = 0.2, random_state = 0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4defbe9",
   "metadata": {
    "id": "a4defbe9",
    "outputId": "c7e5d2a0-0997-4b50-e649-26325936ebbe"
   },
   "outputs": [],
   "source": [
    "# adding the neuron layers \n",
    "# the units in the input layers is equal to the number of unique words\n",
    "# taken three deeper layers of 2000 units each\n",
    "# Relu as activation in the hidden layers\n",
    "# the output layer has 3 units as the one hot encoding has 3 columns\n",
    "# the classification is in categorical\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51e79ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d35a194",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units = 13264, kernel_initializer = 'random_uniform', activation = 'relu'))\n",
    "model.add(Dense(units = 2000, kernel_initializer = 'random_uniform', activation = 'relu'))\n",
    "model.add(Dense(units = 2000, kernel_initializer = 'random_uniform', activation = 'relu'))\n",
    "model.add(Dense(units = 2000, kernel_initializer = 'random_uniform', activation = 'relu'))\n",
    "model.add(Dense(units = 3, kernel_initializer = 'random_uniform', activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b1eeffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam',loss = 'categorical_crossentropy', metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1b4279a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff32194c640>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,batch_size = 128,epochs = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8673c881",
   "metadata": {
    "id": "8673c881",
    "outputId": "a4acac20-a98f-4add-c460-0872311df2a2"
   },
   "outputs": [],
   "source": [
    "# testing the prediction\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "text =  \"The food is okay. average place \"\n",
    "text = re.sub('[^a-zA-Z]', ' ',text)\n",
    "text = text.lower()\n",
    "text = text.split()\n",
    "text = [ps.stem(word) for word in text if not word in set(stopwords.words('english'))]\n",
    "text = ' '.join(text)\n",
    "\n",
    "y_p = model.predict(cv.transform([text]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c1720c38",
   "metadata": {
    "id": "c1720c38"
   },
   "outputs": [],
   "source": [
    "model.save(\"zomato_2_analysis.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bedd4419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zomato_2_analysis.h5\r\n"
     ]
    }
   ],
   "source": [
    "!tar -zcvf SentimentAnalysis-model_new.tgz zomato_2_analysis.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6cd73517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv.pkl  SentimentAnalysis-model_new.tgz  zomato_2_analysis.h5  zomato.csv\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "04380368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: watson-machine-learning-client in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (1.0.391)\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from watson-machine-learning-client) (4.62.3)\n",
      "Requirement already satisfied: certifi in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from watson-machine-learning-client) (2022.6.15)\n",
      "Requirement already satisfied: tabulate in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from watson-machine-learning-client) (0.8.9)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from watson-machine-learning-client) (2.26.0)\n",
      "Requirement already satisfied: boto3 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from watson-machine-learning-client) (1.18.21)\n",
      "Requirement already satisfied: urllib3 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from watson-machine-learning-client) (1.26.7)\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from watson-machine-learning-client) (1.3.4)\n",
      "Requirement already satisfied: ibm-cos-sdk in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from watson-machine-learning-client) (2.11.0)\n",
      "Requirement already satisfied: lomond in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from watson-machine-learning-client) (0.3.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from boto3->watson-machine-learning-client) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from boto3->watson-machine-learning-client) (0.5.0)\n",
      "Requirement already satisfied: botocore<1.22.0,>=1.21.21 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from boto3->watson-machine-learning-client) (1.21.41)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from botocore<1.22.0,>=1.21.21->boto3->watson-machine-learning-client) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.22.0,>=1.21.21->boto3->watson-machine-learning-client) (1.15.0)\n",
      "Requirement already satisfied: ibm-cos-sdk-core==2.11.0 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from ibm-cos-sdk->watson-machine-learning-client) (2.11.0)\n",
      "Requirement already satisfied: ibm-cos-sdk-s3transfer==2.11.0 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from ibm-cos-sdk->watson-machine-learning-client) (2.11.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from requests->watson-machine-learning-client) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from requests->watson-machine-learning-client) (3.3)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from pandas->watson-machine-learning-client) (2021.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from pandas->watson-machine-learning-client) (1.20.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install watson-machine-learning-client --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "02429c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watson_machine_learning import APIClient\n",
    "wml_credentials = { \"url\": \"https://us-south.ml.cloud.ibm.com\", \n",
    "                  \"apikey\": \"b7BpJij5gFeuL2cR1_s14y1Lb7TssAl94vggb0m3LLqh\" }\n",
    "client = APIClient(wml_credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d63311be",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = APIClient(wml_credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8a100acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def guid_from_space_name(client, space_name):\n",
    "    space=client.spaces.get_details()\n",
    "    return(next(item for item in space['resources'] if item['entity'][\"name\"]== space_name)['metadata']['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d760edf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Space UID 7e164fd0-b9db-4b39-92fe-71d0982f3f26\n"
     ]
    }
   ],
   "source": [
    "space_uid= guid_from_space_name(client, 'SentimentAnalysis')\n",
    "print(\"Space UID \"+ space_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ed9ae07f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SUCCESS'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.set.default_space(space_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4409a272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------  ------------------------------------  ----\n",
      "NAME                           ASSET_ID                              TYPE\n",
      "default_py3.6                  0062b8c9-8b7d-44a0-a9b9-46c416adcbd9  base\n",
      "kernel-spark3.2-scala2.12      020d69ce-7ac1-5e68-ac1a-31189867356a  base\n",
      "pytorch-onnx_1.3-py3.7-edt     069ea134-3346-5748-b513-49120e15d288  base\n",
      "scikit-learn_0.20-py3.6        09c5a1d0-9c1e-4473-a344-eb7b665ff687  base\n",
      "spark-mllib_3.0-scala_2.12     09f4cff0-90a7-5899-b9ed-1ef348aebdee  base\n",
      "pytorch-onnx_rt22.1-py3.9      0b848dd4-e681-5599-be41-b5f6fccc6471  base\n",
      "ai-function_0.1-py3.6          0cdb0f1e-5376-4f4d-92dd-da3b69aa9bda  base\n",
      "shiny-r3.6                     0e6e79df-875e-4f24-8ae9-62dcc2148306  base\n",
      "tensorflow_2.4-py3.7-horovod   1092590a-307d-563d-9b62-4eb7d64b3f22  base\n",
      "pytorch_1.1-py3.6              10ac12d6-6b30-4ccd-8392-3e922c096a92  base\n",
      "tensorflow_1.15-py3.6-ddl      111e41b3-de2d-5422-a4d6-bf776828c4b7  base\n",
      "runtime-22.1-py3.9             12b83a17-24d8-5082-900f-0ab31fbfd3cb  base\n",
      "scikit-learn_0.22-py3.6        154010fa-5b3b-4ac1-82af-4d5ee5abbc85  base\n",
      "default_r3.6                   1b70aec3-ab34-4b87-8aa0-a4a3c8296a36  base\n",
      "pytorch-onnx_1.3-py3.6         1bc6029a-cc97-56da-b8e0-39c3880dbbe7  base\n",
      "pytorch-onnx_rt22.1-py3.9-edt  1d362186-7ad5-5b59-8b6c-9d0880bde37f  base\n",
      "tensorflow_2.1-py3.6           1eb25b84-d6ed-5dde-b6a5-3fbdf1665666  base\n",
      "spark-mllib_3.2                20047f72-0a98-58c7-9ff5-a77b012eb8f5  base\n",
      "tensorflow_2.4-py3.8-horovod   217c16f6-178f-56bf-824a-b19f20564c49  base\n",
      "runtime-22.1-py3.9-cuda        26215f05-08c3-5a41-a1b0-da66306ce658  base\n",
      "do_py3.8                       295addb5-9ef9-547e-9bf4-92ae3563e720  base\n",
      "autoai-ts_3.8-py3.8            2aa0c932-798f-5ae9-abd6-15e0c2402fb5  base\n",
      "tensorflow_1.15-py3.6          2b73a275-7cbf-420b-a912-eae7f436e0bc  base\n",
      "pytorch_1.2-py3.6              2c8ef57d-2687-4b7d-acce-01f94976dac1  base\n",
      "spark-mllib_2.3                2e51f700-bca0-4b0d-88dc-5c6791338875  base\n",
      "pytorch-onnx_1.1-py3.6-edt     32983cea-3f32-4400-8965-dde874a8d67e  base\n",
      "spark-mllib_3.0-py37           36507ebe-8770-55ba-ab2a-eafe787600e9  base\n",
      "spark-mllib_2.4                390d21f8-e58b-4fac-9c55-d7ceda621326  base\n",
      "xgboost_0.82-py3.6             39e31acd-5f30-41dc-ae44-60233c80306e  base\n",
      "pytorch-onnx_1.2-py3.6-edt     40589d0e-7019-4e28-8daa-fb03b6f4fe12  base\n",
      "default_r36py38                41c247d3-45f8-5a71-b065-8580229facf0  base\n",
      "autoai-ts_rt22.1-py3.9         4269d26e-07ba-5d40-8f66-2d495b0c71f7  base\n",
      "autoai-obm_3.0                 42b92e18-d9ab-567f-988a-4240ba1ed5f7  base\n",
      "pmml-3.0_4.3                   493bcb95-16f1-5bc5-bee8-81b8af80e9c7  base\n",
      "spark-mllib_2.4-r_3.6          49403dff-92e9-4c87-a3d7-a42d0021c095  base\n",
      "xgboost_0.90-py3.6             4ff8d6c2-1343-4c18-85e1-689c965304d3  base\n",
      "pytorch-onnx_1.1-py3.6         50f95b2a-bc16-43bb-bc94-b0bed208c60b  base\n",
      "autoai-ts_3.9-py3.8            52c57136-80fa-572e-8728-a5e7cbb42cde  base\n",
      "spark-mllib_2.4-scala_2.11     55a70f99-7320-4be5-9fb9-9edb5a443af5  base\n",
      "spark-mllib_3.0                5c1b0ca2-4977-5c2e-9439-ffd44ea8ffe9  base\n",
      "autoai-obm_2.0                 5c2e37fa-80b8-5e77-840f-d912469614ee  base\n",
      "spss-modeler_18.1              5c3cad7e-507f-4b2a-a9a3-ab53a21dee8b  base\n",
      "cuda-py3.8                     5d3232bf-c86b-5df4-a2cd-7bb870a1cd4e  base\n",
      "autoai-kb_3.1-py3.7            632d4b22-10aa-5180-88f0-f52dfb6444d7  base\n",
      "pytorch-onnx_1.7-py3.8         634d3cdc-b562-5bf9-a2d4-ea90a478456b  base\n",
      "spark-mllib_2.3-r_3.6          6586b9e3-ccd6-4f92-900f-0f8cb2bd6f0c  base\n",
      "tensorflow_2.4-py3.7           65e171d7-72d1-55d9-8ebb-f813d620c9bb  base\n",
      "spss-modeler_18.2              687eddc9-028a-4117-b9dd-e57b36f1efa5  base\n",
      "pytorch-onnx_1.2-py3.6         692a6a4d-2c4d-45ff-a1ed-b167ee55469a  base\n",
      "spark-mllib_2.3-scala_2.11     7963efe5-bbec-417e-92cf-0574e21b4e8d  base\n",
      "-----------------------------  ------------------------------------  ----\n",
      "Note: Only first 50 records were displayed. To display more use 'limit' parameter.\n"
     ]
    }
   ],
   "source": [
    "client.software_specifications.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bff213f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'acd9c798-6974-5d2f-a657-ce06e986df4d'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "software_spec_uid= client.software_specifications.get_uid_by_name(\"tensorflow_rt22.1-py3.9\")\n",
    "software_spec_uid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9e77a56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This method is deprecated, please use get_model_id()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/Python-3.9/lib/python3.9/site-packages/ibm_watson_machine_learning/repository.py:1452: UserWarning: This method is deprecated, please use get_model_id()\n",
      "  warn(\"This method is deprecated, please use get_model_id()\")\n"
     ]
    }
   ],
   "source": [
    "model_details = client.repository.store_model(model='SentimentAnalysis-model_new.tgz', meta_props={\n",
    "    client.repository.ModelMetaNames.NAME:\"CNN\",\n",
    "    client.repository.ModelMetaNames.TYPE:\"tensorflow_2.7\",\n",
    "    client.repository.ModelMetaNames.SOFTWARE_SPEC_UID:software_spec_uid\n",
    "}) \n",
    "model_id= client.repository.get_model_uid(model_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0b95c412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'03e9816e-0f49-497a-9aff-f78f13da4189'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a08861eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved model content to file: 'CNN'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/wsuser/work/CNN'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.repository.download(model_id,'CNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e10711",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
