{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "787ec031",
   "metadata": {
    "id": "787ec031"
   },
   "outputs": [],
   "source": [
    "#import Libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f24ebfaf",
   "metadata": {
    "id": "f24ebfaf",
    "outputId": "23997a6b-ff2e-4837-e97c-1e425f9affff"
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17932/3021805726.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#First import the dataset in the dataset variable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#data_review variable contains the reveiws and ratings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"zomato.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdata_review\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'reviews_list'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 488\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    489\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1045\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1046\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1047\u001b[1;33m         \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1048\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1049\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m                 \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m                 \u001b[1;31m# destructive to chunks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._string_box_utf8\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#First import the dataset in the dataset variable\n",
    "#data_review variable contains the reveiws and ratings \n",
    "dataset = pd.read_csv(\"zomato.csv\")\n",
    "data_review = dataset['reviews_list']\n",
    "\n",
    "print(dataset['reviews_list'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b660c854",
   "metadata": {
    "id": "b660c854"
   },
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c689df98",
   "metadata": {
    "id": "c689df98"
   },
   "outputs": [],
   "source": [
    "# here we tokenize the rating string and the review string\n",
    "#loop over all the rows\n",
    "for row_num in range(0,51717):\n",
    "    # split the revie text at '()\n",
    "    lst = data_review[row_num].split(\"('\")\n",
    "    for i in lst:\n",
    "        if len(i) > 5:\n",
    "            if i.find(\"',\") != -1:\n",
    "                single_rev = i.split(\"',\")\n",
    "                if len(single_rev[0]) > 2:\n",
    "                    x.append(single_rev[0])\n",
    "                if len(single_rev[1]) > 2:    \n",
    "                    y.append(single_rev[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95545bfe",
   "metadata": {
    "id": "95545bfe"
   },
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8223c1c",
   "metadata": {
    "id": "b8223c1c",
    "outputId": "4329987e-0714-4e11-8229-ea99780dc126"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alekhya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Import the Librariesimport re \n",
    "import nltk\n",
    "nltk.download(\"stopwords\") \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cdfd667",
   "metadata": {
    "id": "0cdfd667"
   },
   "outputs": [],
   "source": [
    "# to store the final rating\n",
    "rating_final = []\n",
    "# to store cleaned revies\n",
    "review_final = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b639a0d",
   "metadata": {
    "id": "3b639a0d"
   },
   "outputs": [],
   "source": [
    "# the rating string contains words and numbers \n",
    "# so we tokenize the numbers only from it and change into float\n",
    "# for rartings below 2.5 we store the rating as poor\n",
    "# for ratings between 2.5 and 3.5 the rating as average\n",
    "# for ratings more than 3.5 tha rating stored as good\n",
    "\n",
    "for loop in range(0,40000):\n",
    "    data_x = x[loop]\n",
    "    data_x = re.sub('[a-zA-Z]', \" \", data_x)\n",
    "    data_x = data_x.split()\n",
    "    data_x = ''.join(data_x)\n",
    "    data_x = float(data_x)\n",
    "    if data_x < 2.5:\n",
    "        rating_final.append(\"poor\") #poor\n",
    "    elif data_x >= 2.5 and data_x <= 3.5 :    \n",
    "        rating_final.append(\"average\") # average\n",
    "    elif data_x > 3.5:\n",
    "        rating_final.append(\"good\") #good\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa8815ea",
   "metadata": {
    "id": "fa8815ea",
    "outputId": "1e69d796-e51e-4281-d65b-328a794c6526"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in c:\\users\\alekhya\\anaconda3\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\alekhya\\anaconda3\\lib\\site-packages (from sklearn) (0.24.2)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\alekhya\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.20.3)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\alekhya\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.7.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\alekhya\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\alekhya\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc115ca5",
   "metadata": {
    "id": "bc115ca5"
   },
   "outputs": [],
   "source": [
    "# label encode the Ratings and OneHotEncode for the classification        \n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "rating_final = le.fit_transform(rating_final)\n",
    "\n",
    "rating_final = np.array(rating_final)\n",
    "rating_final = np.expand_dims(rating_final, axis=1)\n",
    "        \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "one = OneHotEncoder()\n",
    "rates = one.fit_transform(rating_final).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdc69493",
   "metadata": {
    "id": "bdc69493"
   },
   "outputs": [],
   "source": [
    "# Here the unnecessary stop words from the reviews lists\n",
    "# Stemming operations are also done here \n",
    "\n",
    "for loop in range(0,40000) : \n",
    "    data_y = y[loop]\n",
    "    data_y = re.sub('[^a-zA-Z]', \" \", data_y)\n",
    "    data_y = data_y.lower()\n",
    "    data_y = data_y.split()\n",
    "    data_y = [ps.stem(word) for word in data_y if not word in set(stopwords.words('english'))]\n",
    "    data_y = ' '.join(data_y)\n",
    "    review_final.append(data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a2262f5",
   "metadata": {
    "id": "0a2262f5"
   },
   "outputs": [],
   "source": [
    "# count vectorize the reviews according to the unique words    \n",
    "                    \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features = 20000)\n",
    "x_final = cv.fit_transform(review_final).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5e74e06",
   "metadata": {
    "id": "a5e74e06"
   },
   "outputs": [],
   "source": [
    "# saving the vectorizer which would be used as dictionary.\n",
    "import pickle\n",
    "pickle.dump(cv, open('cv.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fb4497c",
   "metadata": {
    "id": "5fb4497c"
   },
   "outputs": [],
   "source": [
    "# Split the data into test and train sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_final,rates, test_size = 0.2, random_state = 0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4defbe9",
   "metadata": {
    "id": "a4defbe9",
    "outputId": "c7e5d2a0-0997-4b50-e649-26325936ebbe"
   },
   "outputs": [],
   "source": [
    "# adding the neuron layers \n",
    "# the units in the input layers is equal to the number of unique words\n",
    "# taken three deeper layers of 2000 units each\n",
    "# Relu as activation in the hidden layers\n",
    "# the output layer has 3 units as the one hot encoding has 3 columns\n",
    "# the classification is in categorical\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "model = Sequential()\n",
    "model.add(Dense(units = 13264, kernel_initializer = 'random_uniform', activation = 'relu'))\n",
    "model.add(Dense(units = 2000, kernel_initializer = 'random_uniform', activation = 'relu'))\n",
    "model.add(Dense(units = 2000, kernel_initializer = 'random_uniform', activation = 'relu'))\n",
    "model.add(Dense(units = 2000, kernel_initializer = 'random_uniform', activation = 'relu'))\n",
    "model.add(Dense(units = 3, kernel_initializer = 'random_uniform', activation = 'softmax'))\n",
    "model.compile(optimizer = 'adam',loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "model.fit(x_train,y_train,batch_size = 128,epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8673c881",
   "metadata": {
    "id": "8673c881",
    "outputId": "a4acac20-a98f-4add-c460-0872311df2a2"
   },
   "outputs": [],
   "source": [
    "# testing the prediction\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "text =  \"The food is okay. average place \"\n",
    "text = re.sub('[^a-zA-Z]', ' ',text)\n",
    "text = text.lower()\n",
    "text = text.split()\n",
    "text = [ps.stem(word) for word in text if not word in set(stopwords.words('english'))]\n",
    "text = ' '.join(text)\n",
    "\n",
    "y_p = model.predict(cv.transform([text]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1720c38",
   "metadata": {
    "id": "c1720c38"
   },
   "outputs": [],
   "source": [
    "model.save(\"zomato_2_analysis.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04380368",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
